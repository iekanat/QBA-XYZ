{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Logistic and Multinomial Regression\"\nauthor: \"Irfan Kanat\"\ndate: \"September 15, 2017\"\noutput:\n  pdf_document: default\n  html_document: default\ngeometry: margin=1in\nurlcolor: blue\n---\n\nQuick note before we start. This is the first module we use the caret package. The package itself is quite large. So I recommend you install it before you do anything else.\n\n```{r setup, include=F}\ndata(mtcars)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(caret)\nlibrary(pROC)\nlibrary(plotROC)\n```\n\nIn the previous module we learned about regression, with a continuous dependent variable. In this learning activity we will extend the regression to categorical variables[^1]. We will start with case of binary variables, and expand upon it.\n\n[^1]: A categorical variable is one that takes discrete levels. Gender, Eye Color, Transmission Type (automatic = 0 or manual = 1), or Neighborhood can be considered categorical. We call categorical variables with only two levels as binary variables. Transmission type or pyhsiological gender can be considered binary.\n\n# What is the Big Idea?\n\nLet us start with a brief discussion of why we can not simply fit an OLS regression to these variables[^2]. \n\n[^2]: In fact, we can fit OLS to binary variables, these models are called continuous probability models but their use is not very common.\n\nLet us assume a binary variable such as gender. The variable can take two levels: male or female. If you compare this to the continuous variables we used in OLS, you will quickly realize that things are a bit different. Let us look at how binary variables look compared to continuous variables.\n\nI will use the now familiar mtcars dataset. We have used this dataset since the first module, so you should recognize it. I am plotting the type of transmission (Automatic/Manual) and miles per galon. The transmission type is binary. \n\n```{r}\nplotContinuous <- qplot(x = seq_along(mpg), y = mpg, data = mtcars) + ggtitle(\"Continous\")\nplotBinary <- qplot(x = seq_along(am), y = am, data = mtcars) + ggtitle(\"Binary\")\ngrid.arrange(plotBinary, plotContinuous, ncol=1)\n```\n\nDespite the difference in the scale of the variables, it is clear that there is a lot more levels a continuous variable can take.\n\nThe problem is evident in the distribution of binary variable. First, a binary variable is restricted to two levels. When you try to use OLS, you will start violating several assumptions. The regression can produce values greater than 1 and smaller than 0. Second, often times the linearity assumption of OLS is too restrictive in binary dependent variables. Third, OLS assumes error term is normally distributed. With a Binary dependent variable that is hard to satisfy. Finally, OLS assumes homoskedasticty (constant variance) in error terms. Again, this assumption is near impossible to satisfy with a binary variable.\n\nWe need to find a way to model the relationship between dependent variable and independent variables without getting caught up with all the above restrictions.\n\n# Logistic Regression\n\nSo the problem is that the dependent variable is bounded and relation between dependent variable and independent variable may be non-linear. One solution is quite simple. Run your OLS and then carry out a non-linear transformation on the results to conform to the boundaries. This is referred to as a link function. The most popular link function for binary variables is the logit transformation. \n\nLogistic regression is the name given to a regression with logit link function?\n\n## How Does Logit Fit in Logistic Regression\n\nWith binary dependent variables, what we do is model the probability of membership in a category. This is sensible as the probability ranges from 0-1 in a continuous fashion, whereas the dependent variable is discrete (0 or 1).\n\nWe can use OLS to estimate the probability, and then transform this estimate to conform to restrictions listed above. The formula for logistic regression is given below:\n\n$log( \\frac{p}{1-p} ) = \\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2$\n\nWith two little changes to OLS formulation, we have the logit regression: (1) on the right hand side we have the link function applied to probabilities, (2) there is no error term (probability incorporates error).\n\n## How Does it Look\n\nA simplified example with one independent variable (mpg) provides a visual key into what we described above. Compare the blue line below to what an OLS would have provided (red). That is the S shape the logit is famous for. It conforms to the bounds and non-linearity we discussed above.\n\n```{r}\nggplot(mtcars, aes(x = mpg, y = am)) + \n    geom_smooth(method=\"glm\", method.args = list(family=\"binomial\"), se=FALSE, color=\"blue\")+\n    geom_smooth(method=\"lm\", se=F,  color=\"red\") +\n# Bonus: rename the y axis label\n\t\tylab('Probability of Manual Transmission')\n```\n\n\n## What is This About Odds, Log Odds and Probabilities?\n\nWhen you are dealing with logit, you will quickly become familiar with odds, log odds and probabilities. Let me provide a quick overview.\n\nThe probability is restricted between 0 and 1... For example, a ten sided die (d10) has 60% to produce a number smaller than or equal to six.\n\nThe odds do not have an upper limit and they are non-linear. The odds of obtaining a number smaller than or equal to six on a d10 is 1.5 (p/(1-p) = .6 / .4).\n\nWhen you take natural logarithm of odds (logit), the lower limit is removed. The log odds of obtaining a number smaller than or equal to six on a d10 is .4 (log(1.5)).\n\nHere is a table to show you how these things work.\n\n```{r}\n# For a sequence of probabilites from .1 to .9\np <- seq(.1, .9, by = .1)\n# Calculate the Odds\nodds <- p/(1-p)\n# Calculate the log odds (logit)\nlnOdds <- log(odds)\n# Present results\ndata.frame(cbind(p, odds, lnOdds))\n```\n\nNote that for probability of .5 the odds are 1 and the log odds are 0.\n\nInspecting the table, you can see that this type of transformation takes care of boundaries (0 and 1) and linearity. With this transformation, we no longer have to worry about violating the OLS assumptions.\n\nStill, we may need to transform logit results to things more understandable to human beings.\n\nTo obtain odds from logit, we exponentiate (reversing the log) both sides.\n\n$\\frac{p}{1-p} = e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}$\n\nFrom odd's we can move onto probabilities like so:\n\n$p = \\frac{1}{1+e^{\\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2}}$\n\nHere is a simple exercise for you. Probability of landing a critical hit (rolling a 20 on a 20 sided die) is 5%. Calculate the odds and log odds of critical hit.\n\nHow about the probability and odds of getting hit by a fireball, if the log odds are .5?\n\n\n## What is the Catch?\n\nNothing in life is free... Once you use a link function, the $\\beta$ coefficients are no longer as easy to interpret. We will see how this translate into interpretation of coefficients later on.\n\n# Logit in R\n\nWe have gone through quite a dry description on why we have logit regression. Let us go over an example to see how all the above discussion fits into practice.\n\n## Fit a Logit Model\n\nLet us fit a logit model to predict transmission type of cars in mtcars dataset.\n\nWe use glm() to fit these kinds of **g**eneralized **l**inear **m**odels. Please skim the manual page for glm (?glm) before virtual office hours.\n\n```{r}\nmtcars_lgt_0 <- glm(am ~ mpg + vs , mtcars, family = \"binomial\")\n```\n\nLet us see what we have done thus far:\n\n1. mtcars_lgt_0 <-: assigns the estimation results into a variable named mtcars_lgt_0.\n2. glm(am ~ mpg + vs, mtcars, family = \"binomial\"): calls lm function with three parameters\n    + am ~ mpg + vs : is the formula specification, left of ~ is the dependent variable, and right of ~ is the independent variables.\n      - Since am = 1 when car is manual, we model the car having a manual transmission.\n    + data = mtcars: name of the dataset to draw the variables from (\"data =\" ommitted). \n    + family = \"binomial\": Tells R to use logit link function\n\n## Evaluate Results\n\nLet us view the results of estimation.\n\n```{r}\nsummary(mtcars_lgt_0)\n```\n\nAs with OLS, the output states what the function call was. This is very useful if you somehow lost the code you used to create estimation.\n\nThen comes the deviance residuals. This is an overview of model fit. Do not worry about these, as we will discuss about model fit below.\n\nCoefficients is what most people would be interested in. It shows the estimates for coefficients, we can see miles per galon is a statistically significant predictor of transmission type. The estimate reported here is the effect on log odds.\n\nBelow this all is the fit indices.\n\n### Evaluate Model Fit\n\nAIC: Among the values reported, the easiest to interpret is AIC. Aikaike's Information Criterion reports the model fit, lower the value, better the fit. It is useful to compare to models fit to the same dataset (not necessarily nested). It's usefulness however is limited to model comparison.\n\nLog Likelihood Test: If you want to make sure the model is actually good, one way is to compare it to a null model (nothing on the right hand side). We can use the null deviance and residual deviance for this purpose. We will compare the fit of our model to see if the model is an improvement over a null model. We will use a chi squared ($\\chi^2$) test for this comparison.\n\nIf you inspect the summary output, below the coefficients, you will see some deviances reported. We can run a chi square test these values. You will note that the null deviance is `r mtcars_lgt_0$null.deviance` with `r mtcars_lgt_0$df.null` degrees of freedom. The residual deviance of our model on the other hand is `r mtcars_lgt_0$deviance` with `r mtcars_lgt_0$df.residual` degrees of freedom. For chi squared ($\\chi^2$) test we will use the difference in deviance with the difference in degrees of freedom. This will tell us if the model is a significant improvement over a null model.\n\nLooking at the results the values are: deviance = 43.23 - 29.94 = 13.29 with df = 31 - 29 = 2. We can then estimate the significance by \npchisq(13.29, 2, lower.tail = F)\n\n```{r}\n# Instead of manually calculating the values, we can also ask R to do it for us\npchisq(mtcars_lgt_0$null.deviance - mtcars_lgt_0$deviance, \n       mtcars_lgt_0$df.null - mtcars_lgt_0$df.residual,\n       lower.tail = F)\n```\n\nP value smaller than .05 means the model is statistically significantly better than null model.\n\n### Evaluate Coefficients\n\nThe beta coefficients R reports are the effect on log odds. These do not have an intuitive interpretation. Best we can say is that each additional mile per galon increases the logged odds of the car being a manual by `r round(mtcars_lgt_0$coefficients[2], digits=2)`.\n\nIf you want odds-ratios, you will need to exponentiate the coefficients. Let us compare raw coefficients with odds-ratios:\n\n```{r}\nmtcars_lgt_0$coefficients\n# Exponentiate the coefficients\nexp(mtcars_lgt_0$coefficients)\n```\n\nThe interpretation of odds-ratios are more straightforward. For one unit increase in mpg, odds of being a manual transmission increase by a ***factor*** of `r round(exp(mtcars_lgt_0$coefficients[2]), digits=2)`.\n\nSince vs is insignificant, we should not interpret it. If it were significant, we would interpret it like any other binary independent variable, but the coefficient would be multiplicative in odds-ratio.\n\nWhile the output has the intercept, it is not meaningful to interpret.\n\n### Model Performance in Prediction\n\nLogit model is also used for predictions. How do we go from what we have done so far to predicting the outcome? \n\nWe can obtain predicted probabilities from the model with predict function.\n\n```{r}\n# Type parameter specifies probability\npredict(mtcars_lgt_0, type=\"response\")\n```\n\nIf we know nothing more about the data we can use .5 as a cut-off and say any car that has a predicted probability greater than .6 is a manual. **If we knew the prevalence of manual transmission, we can make a more informed decision.** If the sample we have is representative of the population, then we can use the prevalence of manual transmission in cars as an indicator.\n\n```{r}\nmean(mtcars$am)\n```\n\nWe know that roughly `r round(mean(mtcars$am), digits=2)` of cars are manual. That means only 40 percent of our predictions should result in a positive prediction. That means we should set our cut-off at .6 (1-.4).\n\nLet us add two variables to the data.frame and go from there.\n\n```{r}\n# Save predicted probabilities\nmtcars$prob <- predict(mtcars_lgt_0, type = \"response\")\n# Create a variable for predicted transmission type\nmtcars$predAM <- 0\n# Filter the cars with predicted probabilities greater than .6\n# and change their prediction to manual.\nmtcars[mtcars$prob > .6, 'predAM'] <- 1\n```\n\nNow we can compare what we predicted to what was actually observed.\n\n```{r}\nmtcars[,c(\"predAM\", \"am\")]\n```\n\nIf the model was perfect, the two columns would match perfectly. Realistically, we can not go through every line and compare the results... There needs to be a better way to find out model performance.\n\n#### Confusion Matrix\n\nOne way to learn about model's predictive performance is a confusion matrix. Easiest way to obtain a confusion matrix is to tabulate predicted and actual category membership data.\n\n```{r}\ntable(mtcars[, c(\"predAM\", \"am\")])\n```\n\nThere are four cells here:\n\nTrue Positives: We predicted 1, and the actual value was 1.\nTrue Negatives: We predicted 0, and the actual value was 0.\nFalse Positives: We predicted 1, when the actual value was 0.\nFalse Negatives: We predicted 0, when the actual value was 1.\n\nFrom these information we can calculate various metrics of performance. I am listing the commonly used ones below.\n\n\nPrevalence: What is the prevalence of positives in sample? ((TP+FN)/Total) \n\nAccuracy: How often were we able to predict correctly? ((TP+TN)/Total) \n\nSpecificity: How good is our model at predicting negatives? (TN/(TN+FP))\n\nSensitivity: How good is our model at predicting positives? (TP/(TP+FN)) \n\nPositive Prediction Rate: How good is our model in predicting positives, taking into account prevalence. $(sensitivity \\times prevalence) / ((sensitivity \\times prevalence) + ((1-specificity) \\times (1-prevalence)))$\n\nNegative Prediction rate: How good is our model in predicting negatives, taking into account prevalence. $(specificity \\times prevalence) / ((specificity \\times prevalence) + ((1-specificity) \\times (1-prevalence)))$\n\nKappa: How well the classifier performed compared to random prediction.\n\nRead the confusionMatrix manual for more on these measures.\n\n```{r}\n# Here we use confusionMatrix function from caret package\nconfusionMatrix(table(mtcars[,c(\"predAM\", \"am\")]))\n```\n\n#### Receiver Operating Characteristics (ROC) Curve\n\nAnother way to evaluate model performance is to look at the ROC curve. It shows the sensitivity/specificity as a function of the cut off value.\n\n```{r}\n# Calculate the ROC curve\nmtcars_am_roc <- roc(am~prob, mtcars) \n# You can call the calculation to see the area under curve\nmtcars_am_roc\n# Or use the plot function to plot the curve\n#plot(mtcars_am_roc)\n# Ggplot with plotROC package does a better job.\nggplot(mtcars, aes(d=am, m=prob))+ geom_roc() + style_roc()\n```\n\nLooking at the curve, the ideal ROC curve would look like a right angle. Where Specificity is equal to 1 and sensitivity (1-false positive rate) is equal to 1. In which case the area under the curve would equal 1. \n\nThe worst case scenario is both sensitivity and specificity being equal to 0. In which case the area under the curve will be equal to 0.\n\nGenerally speaking, if the area under the curve is less than .6 then the model is not good.\n\n\n\n# Solutions to Exercises\n\n1 - Calculate the odds and log odds of critical hit.\n\n```{r}\np <- .05\n1/(1-p) # Odds\nlog(1/(1-p)) # Log Odds\n```\n\n2 - Calculate the probability event, if the odds are .3?\n\n```{r}\nlnOdds <- .5\nexp(lnOdds) # Odds\nexp(lnOdds)/(1+exp(lnOdds)) # Probability\n```",
    "created" : 1505526680771.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "153219534",
    "id" : "C3474710",
    "lastKnownWriteTime" : 1507511730,
    "last_content_update" : 1507511730924,
    "path" : "~/Dropbox/QBAXXXX/R Content/Module 4 - Classification Models/1_Logit.Rmd",
    "project_path" : "1_Logit.Rmd",
    "properties" : {
        "last_setup_crc32" : "6C93A6AEe49f8c17",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}