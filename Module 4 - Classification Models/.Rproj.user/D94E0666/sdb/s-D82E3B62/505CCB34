{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Example Analysis\"\nauthor: \"Irfan Kanat\"\ndate: \"11/04/2015\"\noutput:\n  pdf_document:\n    fig_height: 3\n    fig_width: 4\n    keep_tex: yes\n---\n\n\n## Statistical Models\n\nIn this section, I will try to provide an introduction to using two simple statistical models in R: regression and logistic regression.\n\n### Regression\n\nIf your dependent variable is continuous you can simply use regression.\n\nFor this demonstration, I will use the same Motor Trends dataset I used in Visualization section. \n\n```{r}\ndata(mtcars) # Get the data\n?mtcars # Help on dataset\n```\n\nWe will use lm() function to fit regular regression.\n\n```{r}\n?lm\n```\n\nBelow I declare a model where I use horse power, cylinders, and transmission type to estimate gas milage. Pay attention to model specification:\n\n```\nmpg ~ hp + cyl + am\n```\n\nHere the left hand side of the tilde is the dependent variable. and the right hand side has all the predictors we use separated by plus signs.\n\n```{r}\n# Fit \nreg_0 <- lm( mpg ~ hp + cyl + am, data = mtcars) \nsummary(reg_0)\n```\n\nLook at the R-squared value to see how much variance is explained by the model, the more the better.\n\nYou can access estimated values as follows. I used a head function to limit the output.\n\n```{r}\nhead(reg_0$fitted.values)\n```\n\nYou can use the fitted model to predict new datasets. Here I am modifying Datsun710 to see how the gas milage may have been influenced if the car was automatic instead of manual transmission.\n\n```{r}\nnewCar <- mtcars[3,] # 3rd observation is Datsun 710\nnewCar$am <- 0 # What if it was automatic?\npredict(reg_0, newdata = newCar) # Estimate went down by 4 miles\n```\n\nOne way to see how your model did is to plot residuals. Ideally the residuals should be close to 0 and randomly distributed. If you see a pattern, it indicates misspecification.\n\n```{r}\nlibrary(ggplot2)\n# Plot the fitted values against real values\nqplot(data=mtcars, x = mpg, y = reg_0$residuals) +\n  stat_smooth(method = \"lm\", col = \"red\")\n\n# Are the residuals normally distributed? \nshapiro.test(reg_0$residuals) # yes\n```\n\nComparing models. If you are using the same dataset, and just adding or removing variables to a model. You can compare models with a likelihood ratio test or an F test. Anova facilitates comparison of simple regression models.\n\n```{r}\n# Add variable wt\nreg_1 <- lm( mpg ~ hp + cyl + am + wt, mtcars)\n\n# Aikikae Information Criteria\n# AIC lower the better\nAIC(reg_0)\nAIC(reg_1)\n\n# Compare\nanova(reg_0, reg_1) # models are significantly different\n```\n\n### Logistic Regression\n\nLet us change gears and try to predict a binary variable. For this purpose we will use the logistic regression with a binomial link function. The model estimates the probability of Y=1.\n\nLet us stick to the mtcars dataset and try to figure out if a car is automatic or manual based on predictors.\n\nWe will use glm function.\n\n```{r}\n?glm\n```\n\nLet us fit the model\n\n```{r}\nlogit_2 <- glm(am ~ mpg + drat + cyl, data = mtcars, family='binomial')\nsummary(logit_2)\n```\n\nVisualize the results.\n\n```{r}\nggplot(mtcars, aes(x = mpg, y = am)) + \n    stat_smooth(method=\"glm\", family=\"binomial\", se=FALSE)+\n# Bonus: rename the y axis label\n\t\tylab('Probability of Manual Transmission')\n```\n\nHow about plotting results for number of cylinders? We will need to process the data a little bit.\n\n```{r}\n# Create a new dataset with varying number of cylinders and other variables fixed at mean levels.\nmtcars2<-data.frame(mpg = rep(10:30, 3),drat = mean(mtcars$drat), disp = mean(mtcars$disp), cyl = rep(c(4,6,8),21))\n# Predict probability of new data\nmtcars2$prob<-predict(logit_2, newdata=mtcars2, type = \"response\")\n\n# Plot the results\nggplot(mtcars2, aes(x=mpg, y=prob)) +\n  geom_line(aes(colour = factor(cyl)), size = 1) \n```\n\nDiagnostics with logistic regression.\n\n```{r}\nlibrary(caret)\n\n# Let us compare predicted values to real values\nmtcars$prob <- predict(logit_2, type=\"response\")\n# Prevalence of Manual Transmission\nmean(mtcars$am)\n\n# Create predict variable\nmtcars$pred <- 0\n# If probability is greater than .6 (1-prevalence), set prediction to 1\nmtcars[mtcars$prob>.6, 'pred'] <- 1\n\n# Confusion Matrix\nconfusionMatrix(table(mtcars[,c(\"am\", \"pred\")]))\n\n## ROC CURVE\n# Load the necessary library\nlibrary(pROC)\n# Calculate the ROC curve using the predicted probability vs actual values\nlogit_2_roc <- roc(am~prob, mtcars) \n# Plot ROC curve\nplot(logit_2_roc)\n```\n\n## Caret Package\n\nCaret package is a wrapper that brings together functionality from 27 packages. Caret supports [estimating over 150 models](http://topepo.github.io/caret/bytag.html) including bayesian, SVM, discriminant analysis, regressions, neural networks, and more.\n\n```{r, eval=FALSE}\n# Run below commands to get a list of related packages\navailable.packages()[\"caret\",\"Depends\"]\navailable.packages()[\"caret\",\"Suggests\"]\n```\n\nCaret package aims to be the go to package for your predictive analytics needs. Thus it not only covers model training, but also data manipulation, visualization, and parallelization capabilities.\n\nSince so many packages involved, the installation takes a while.\n\n```{r ,eval=FALSE}\ninstall.packages(\"caret\", dependencies = c(\"Depends\", \"Suggests\"))\n```\n\nThe main idea is comparing performance of alternate models and finding the best fit. Clarification of ambiguities: \n\n 1. Alternate models: test various model parameters\n 2. Best fit: according to various metrics\n 3. Compare performance: through resampling.\n\n## Model Training - Classification Trees with caret Package\n\nIn this section, I will be presenting a classification tree with caret package. The example presented here follows closely [Kuhn's UseR 2013 presentation](https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf). I had to leave out significant portions of Kuhn's work to fit it into the scope of our workshop. Please refer to the original material for details.\n\n\n### Data\n\nWe will use segmentationData from the caret package.\n\n```{r}\nlibrary(caret) # Load necessary library\ndata(segmentationData) # Obtain Dataset\n```\n\nWe do not need the Cell identifiers.\n\n```{r}\n# Drop one column\nsegmentationData <- segmentationData[,!(colnames(segmentationData)=='Cell')] \n```\n\nThe data has a variable \"Case\" to indicate Training, vs Testing observations. We will obtain subsets based on this variable.\n\n```{r}\n# Data set has a variable that separates training vs testing\nTraining <- segmentationData[segmentationData$Case == 'Train', ] #$ One Way\nTesting <- subset(segmentationData, Case == 'Test') # Another way\n# Drop the now defunct Case variable\nTraining <- Training[,!(colnames(Training)=='Case')] # Drop Case column\nTesting <- subset(Testing, select=-c(Case)) # Another way\n```\n\n### Classification Trees without Caret\n\nI want to demonstrate how Classification Trees can be fit without caret first, so that the contribution of caret's train function becomes clearer.\n\nLet us first fit a tree of a limited depth.\n\n```{r, fig.height=6, fig.width=8}\nlibrary(rpart) # Load necessary library\nrpart_1 <- rpart(Class ~ ., data = Training, \n  \t\t\tcontrol = rpart.control(maxdepth=2)) # Fit a shallow tree \nrpart_1 # View results\n\nplot(rpart_1) # Visualization\ntext(rpart_1) # Text\n```\n\nWhen you fit a tree with rpart, it conducts as many spits as possible, then use 10 fold cross validation to prune the tree.\n\n```{r}\n# Fit a larger tree and prune it \n# Rpart does 10 fold cross validation\nrpart_2 <- rpart(Class ~ ., data = Training) \n```\n\npartykit package provides beter plots for classification trees. \n\n```{r, fig.height=8, fig.width=10}\nlibrary(partykit)\nplot(as.party(rpart_2))\n```\n\n```{r}\n# Validate the model\nrpart_2Test <- predict(rpart_2, Testing, type='class')\nconfusionMatrix(rpart_2Test, Testing$Class) # Get the benchmark$\n```\n\n\n## Model Tuning - Classification Trees with Caret\n\ncaret package allows you to change tuning parameters and resampling strategies for the models. Below we instruct caret to use ROC curve with k-fold cross validation repeated 3 times to pick the best model.\n\n```{r}\nlibrary(caret)\n##  Set Training Parameters\n# Triple Cross Validation\ncvCtrl <- trainControl(method = \"repeatedcv\", repeats = 3, # K-fold cross validation repeated thrice\n  \t\t\t\tsummaryFunction = twoClassSummary, classProbs = TRUE) # Class probabilities for ROC\n\n# Use caret to fit a model and fine tune the fit\nCarrotTree <- train(Class ~., data=Training, method='rpart', \n\t\t\t\t\ttrControl=cvCtrl, metric = \"ROC\", tuneLength=30) # Change metric to ROC\n```\n\nLet us see the performance of models over various levels of tuning parameter.\n\n```{r}\nplot(CarrotTree)\n```\n\nLet us see the final model\n\n```{r, fig.height=8, fig.width=10, eval=FALSE}\n# Output omited to conserve space\nplot(as.party(CarrotTree$finalModel)) \n```\n\n\n### Validate Using Testing Data\n\nPredict the class membership with test data and then plot out the confusion matrix for performance metrics of this model over test data.\n\n```{r}\n# Testing\nCarrotTree_Test <- predict(CarrotTree, Testing)\nhead(CarrotTree_Test) # Compare\nhead(Testing$Class) # and Contrast\n# Evaluate Model Performance\nconfusionMatrix(CarrotTree_Test, Testing$Class) \n```\n\n\n## Model Training - Discriminant Analysis with caret Package\n\nIn this section I will present a classification example using caret package. The example presented here follows closely [the caret package vignette](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf). I only made a few changes to fit it into the workshop's time frame. So if you need any clarification, you can read up more in the vignette.\n\n\n### Data\n\nWe will use the sonar dataset from mlbench package. It has 208 observations of 60 predictor variables and a binary class variable as dependent. We do not know what these 60 variables are.\n\n```{r}\n# Dataset comes with mlbench package\nlibrary(mlbench)\n# Load dataset into the current workspace\ndata(Sonar)\n```\n\n### SPLIT THE DATA\n\nThis dataset is not conveniently split as the previous set was, we need to create our own subsets. Caret provides functionality to split the data into a training and a testing set while at the same time preserving the distribution of dependent variable.\n\ncreateDataPartition function receives the dependent variable, proportion of training set in the whole of dataset as parameters.\n\n```{r}\nlibrary(caret) # Load Library\nset.seed(107)  # Set random number seed for reproducibility\n# Create an index of observations to be included in Training\nindexTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)\n\n# Split the data using the index\nTrain <- Sonar[indexTrain,]\nTest  <- Sonar[-indexTrain,]\n```\n\n### Training a Discriminant Model\n\nWe will be using a PLS DA model to train.\n\nFirst step is to set resampling and validation strategy. Here we are using 3 resamplings of 10-fold cross validation. We also configure the trainer to produce predicted probabilities to be used in ROC calculation.\n\n```{r}\n## Declare Tuning Control Parameters\nctrl <- trainControl(method = \"repeatedcv\", # K-fold cross validation\n\t\t\t        repeats = 3, # Repeat resampling 3 times\n                    classProbs = TRUE, # Calculate predicted prob for ROC\n                    summaryFunction = twoClassSummary) # Set performance metrics for binary\n```\n\nBelow we train the model with varying parameters. The tune length in this case specifies maximum the number of components to be extracted.\n\n```{r}\nplsFit <- train(Class ~ ., data = Train, method = \"pls\",\n\t\ttuneLength = 10,   # Number of component sets to be evaluated (more is better)\n\t\ttrControl = ctrl, # Use control parameters from above\n\t\tmetric = \"ROC\" ,  # Criteria ROC\n\t\tpreProc = c(\"center\", \"scale\")) # Center and scale the predictors\n```\n\nLet us review the models that were fit.\n\n```{r}\nplsFit\n```\n\nAs you can see the area under the ROC curve increases up till 4 components and starts declining afterwards. By default the model with 4 components is used.\n\nPlotting this fit would demonstrate the change in area under ROC for different components.\n\n```{r}\n# evaluate the performance of different number of components extracted\nplot(plsFit)\n```\n\n### Validate with Test Data\n\nLet us see how our fitted model performs with Test data.\n\n```{r}\nplsPredict <- predict(plsFit, newdata = Test) # Predict results\nhead(plsPredict) # View predictions\nhead(Test$Class) # View actual$\n```\n\nGet confusion matrix (predicted vs actual) for performance reports.\n\n```{r}\nconfusionMatrix(data = plsPredict, Test$Class)\n```\n\n------\n\n![Creative Commons 4](figures/cc.png) How I Learned to Stop Worrying and Love the R Console by [Irfan E Kanat](http://irfankanat.com) is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/). Based on a work at [http://github.com/iekanat/rworkshop](http://github.com/iekanat/rworkshop).",
    "created" : 1507481840842.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1202283638",
    "id" : "505CCB34",
    "lastKnownWriteTime" : 1447522628,
    "last_content_update" : 1507506961192,
    "path" : "~/Dropbox/R_Workshop/WorkshopR/6_Modeling.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}