{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Fixed Effects Regression\"\nauthor: \"Irfan Kanat\"\ndate: \"August 14, 2017\"\noutput:\n  html_document: default\n  pdf_document: default\n---\n\n```{r setup, include=FALSE}\nlibrary(ggplot2)\nlibrary(plm)\nset.seed(7)\n# Create a simulated dataset\nrent2 <- data.frame(beds = sample(1:5, 300, replace = T),\n                    baths = sample(1:2, 300, replace = T),\n                    sqft = round(rnorm(300, mean = 1000, sd = 200)),\n                    hood = sample(LETTERS[1:3], 300, replace = T))\n# Calculate the rent based on other variables\nrent2$rent <- 200 + 150 * rent2$beds + .50 * rent2$sqft + 50 * rent2$baths -\n  30 * as.numeric(rent2$hood) + rnorm(300, mean = 0, sd = 80)\n# Order the dataset by number of beds\nrent2 <- rent2[order(rent2$beds, rent2$sqft), ]\n```\n\nOne of the assumptions of OLS is that observations are independent of each other. Like coin tosses. The outcome of each toss is independent of the previous toss. We assume that individuals heights are not influenced by other individuals.\n\nThere are certain instances where this independence assumption is violated. \n\nBelow are some examples:\n\nAn individual's height in one year will be highly correlated to his height on previous years. If your dataset tracks multiple observations from same individual, we can not say the observations are independent.\n\nIf you track sales of a product over time, the sales performance in each period will be correlated to previous periods' performance.\n\nThe ACT scores of different students from one school may be correlated due to quality of education in that school.\n\nThe height of trees from one region may be correlated due to weather conditions in that region.\n\nIn all these instances we can not claim the observations are independent. There are various ways to address this problem. Here I will touch on a simple solution.\n\n# Idea Behind Fixed Effects\n\nFrom a linear relation perspective, how would a common factor manifest itself? Let us remember the formula for regression.\n\n$y = \\beta_0 + \\beta_1 x_1 + . . . + \\beta_n x_n + \\epsilon$\n\nWe assume there is an intercept, and a slope for each variable, and an error term. If there is a common factor that we do not observe in the model, the variance caused by that factor will end up in the error term $\\epsilon$.\n\nLet us say we are talking about rent in different neighborhoods. The rent for houses in a neighborhood will be correlated depending on the attractiveness of the neighborhood. A neighborhood with a good school district may command several hundred dollars more per month.\n\nSo, can we say that each neighborhood will have a different intercept? Breaking down the error term $\\epsilon$ into the intercept for neighborhood ($\\alpha_{hood}$) plus some random error ($u$)? $\\alpha_{hood}$ will capture that several hundred dollar difference due to the good school, nearby park, etc.\n\n$y = \\beta_0 + \\beta_1 x_1 + . . . + \\beta_n x_n + \\alpha_{hood} + u$\n\nGenerally speaking, if your independence assumption is violated it will be harder to get significant results as the error term (and residual variance captured by it) will be inflated.\n\nI created a version of rent dataset from earlier that is suitable for our purposes here.\n\n```{r}\nqplot(data = rent2, x = beds, y = rent, col = hood) +\n  geom_smooth(method = \"lm\", se = F)\n```\n\nEach neigborhood follows a similar tracectory (slopes are the same) but the starting points (intercept) are different.\n\n## How Can We Fit This Model?\n\nIf the basic idea is that there is a separate intercept for each grouping (neighborhood, person, company...) then we can use dummy variables.\n\nLet us say you have three neighborhoods: A, B, and C. To capture the information in three (n) levels in binary variables you would need two (n-1) binary variables\n\nA = 0 0\n\nB = 1 0\n\nC = 0 1\n\nVery often in practice we call the Level captured as 0 0 the reference level and label the binary variables after the remaining levels.\n\nSo the data: \n\n```{r}\nhead(rent2, n = 5)\n```\nBecomes:\n\n```{r}\ndata.frame(beds = rent2$beds[1:5], baths = rent2$baths[1:5],\n           sqft = rent2$sqft[1:5], b = c(0, 1, 0, 0, 0), c = c(1, 0, 1, 0, 0),\n           rent = rent2$rent[1:5])\n```\n\nYou won't need to do this by hand, I am just demonstrating the effects of dummy coding a categorical variable.\n\nIf you fit a model with these dummy variables, it is no different than any other model with binary variables. Remember the example of gender from Interactions learning activity.\n\nLet us fit a model with dummies for neigborhood.\n\n```{r}\n # I use factor to declare categorical variables\nrent2_lm_0 <- lm(rent ~ beds + baths + sqft + factor(hood), data = rent2)\nsummary(rent2_lm_0)\n```\n\nLooking at the output you will notice that neighborhood A is not shown in results. That is because it is used as reference level.\n\nAll other dummies will be in relation to reference level.\n\nSo we will say, houses in neighborhood B are `r unname(round(rent2_lm_0$coefficients[5], digits=2))` cheaper than those in neigborhood A.\n\nHouses in neighborhood C are `r unname(round(rent2_lm_0$coefficients[6], digits=2))` cheaper than those in neighborhood A.\n\nThe intercept for neighborhood B will then be $\\beta_0 + \\alpha_B =$ `r unname(round(rent2_lm_0$coefficients[1]+rent2_lm_0$coefficients[5], digits=2))`.\n\nHere is a simple exercise for you, calculate the intercept for neighborhoods A and C.\n\n## More Efficiency\n\nVery often you would not care about the actual value of these fixed effects. All you want is to remove the variance caused by these fixed effects from the model and reduce type 2 error (rejecting a significant relationship).\n\nIf that is all you care about, then you can simply remove these intercepts by \"demeaning\". Substracting the neighborhood average from each observation would essentially remove the effect of neighborhood.\n\nLuckily you won't need to do this by hand. The excellent plm package handles fixed effects models nicely. Good thing, that you DO KNOW how to install and activate packages.\n\n```{r}\nrent2_plmFE_0 <- plm(rent ~ beds + baths + sqft, data = rent2,\n                     # model=\"within\" for fixed effects\n                     # index = 'hood' for clustering variable\n                     model = \"within\", index = \"hood\")\nsummary(rent2_plmFE_0)\n```\n\nIf you compare the coefficients, you will see that the demeaned model is identical to dummy variables model.\n\nThere are many other ways of handling the violation of \"Independence of Observations\" assumption, basically anything more would turn into a full course rather than a learning activity. If you are interested, look into random effects, mixed effects, and Generalized Method of Moments models.\n\n# Solutions to Exercises\n\n1 - Calculate the intercept for neighborhoods A and C.\n\nIntercept for Neighborhood A would be $\\beta_0 =$ `r unname(round(rent2_lm_0$coefficients[1], digits=2))`.\n\nThe intercept for neighborhood C will then be $\\beta_0 + \\alpha_C =$ `r unname(round(rent2_lm_0$coefficients[1]+rent2_lm_0$coefficients[6], digits=2))`.",
    "created" : 1502734327592.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "510200493",
    "id" : "31BAE8ED",
    "lastKnownWriteTime" : 1503075220,
    "last_content_update" : 1503075220330,
    "path" : "~/Dropbox/QBAXXXX/R Content/Module 3 - Continous Modelling/5_Fixed_Effects.Rmd",
    "project_path" : "5_Fixed_Effects.Rmd",
    "properties" : {
        "last_setup_crc32" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}