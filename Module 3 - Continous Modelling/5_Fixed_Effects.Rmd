---
title: "Fixed Effects Regression"
author: "Irfan Kanat"
date: "August 14, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(ggplot2)
library(plm)
set.seed(7)
# Create a simulated dataset
rent2 <- data.frame(beds = sample(1:5, 300, replace = T),
                    baths = sample(1:2, 300, replace = T),
                    sqft = round(rnorm(300, mean = 1000, sd = 200)),
                    hood = sample(LETTERS[1:3], 300, replace = T))
# Calculate the rent based on other variables
rent2$rent <- 200 + 150 * rent2$beds + .50 * rent2$sqft + 50 * rent2$baths -
  30 * as.numeric(rent2$hood) + rnorm(300, mean = 0, sd = 80)
# Order the dataset by number of beds
rent2 <- rent2[order(rent2$beds, rent2$sqft), ]
```

One of the assumptions of OLS is that observations are independent of each other. Like coin tosses. The outcome of each toss is independent of the previous toss. We assume that individuals heights are not influenced by other individuals.

There are certain instances where this independence assumption is violated. 

Below are some examples:

An individual's height in one year will be highly correlated to his height on previous years. If your dataset tracks multiple observations from same individual, we can not say the observations are independent.

If you track sales of a product over time, the sales performance in each period will be correlated to previous periods' performance.

The ACT scores of different students from one school may be correlated due to quality of education in that school.

The height of trees from one region may be correlated due to weather conditions in that region.

In all these instances we can not claim the observations are independent. There are various ways to address this problem. Here I will touch on a simple solution.

# Idea Behind Fixed Effects

From a linear relation perspective, how would a common factor manifest itself? Let us remember the formula for regression.

$y = \beta_0 + \beta_1 x_1 + . . . + \beta_n x_n + \epsilon$

We assume there is an intercept, and a slope for each variable, and an error term. If there is a common factor that we do not observe in the model, the variance caused by that factor will end up in the error term $\epsilon$.

Let us say we are talking about rent in different neighborhoods. The rent for houses in a neighborhood will be correlated depending on the attractiveness of the neighborhood. A neighborhood with a good school district may command several hundred dollars more per month.

So, can we say that each neighborhood will have a different intercept? Breaking down the error term $\epsilon$ into the intercept for neighborhood ($\alpha_{hood}$) plus some random error ($u$)? $\alpha_{hood}$ will capture that several hundred dollar difference due to the good school, nearby park, etc.

$y = \beta_0 + \beta_1 x_1 + . . . + \beta_n x_n + \alpha_{hood} + u$

Generally speaking, if your independence assumption is violated it will be harder to get significant results as the error term (and residual variance captured by it) will be inflated.

I created a version of rent dataset from earlier that is suitable for our purposes here.

```{r}
qplot(data = rent2, x = beds, y = rent, col = hood) +
  geom_smooth(method = "lm", se = F)
```

Each neigborhood follows a similar tracectory (slopes are the same) but the starting points (intercept) are different.

## How Can We Fit This Model?

If the basic idea is that there is a separate intercept for each grouping (neighborhood, person, company...) then we can use dummy variables.

Let us say you have three neighborhoods: A, B, and C. To capture the information in three (n) levels in binary variables you would need two (n-1) binary variables

A = 0 0

B = 1 0

C = 0 1

Very often in practice we call the Level captured as 0 0 the reference level and label the binary variables after the remaining levels.

So the data: 

```{r}
head(rent2, n = 5)
```
Becomes:

```{r}
data.frame(beds = rent2$beds[1:5], baths = rent2$baths[1:5],
           sqft = rent2$sqft[1:5], b = c(0, 1, 0, 0, 0), c = c(1, 0, 1, 0, 0),
           rent = rent2$rent[1:5])
```

You won't need to do this by hand, I am just demonstrating the effects of dummy coding a categorical variable.

If you fit a model with these dummy variables, it is no different than any other model with binary variables. Remember the example of gender from Interactions learning activity.

Let us fit a model with dummies for neigborhood.

```{r}
 # I use factor to declare categorical variables
rent2_lm_0 <- lm(rent ~ beds + baths + sqft + factor(hood), data = rent2)
summary(rent2_lm_0)
```

Looking at the output you will notice that neighborhood A is not shown in results. That is because it is used as reference level.

All other dummies will be in relation to reference level.

So we will say, houses in neighborhood B are `r unname(round(rent2_lm_0$coefficients[5], digits=2))` cheaper than those in neigborhood A.

Houses in neighborhood C are `r unname(round(rent2_lm_0$coefficients[6], digits=2))` cheaper than those in neighborhood A.

The intercept for neighborhood B will then be $\beta_0 + \alpha_B =$ `r unname(round(rent2_lm_0$coefficients[1]+rent2_lm_0$coefficients[5], digits=2))`.

Here is a simple exercise for you, calculate the intercept for neighborhoods A and C.

## More Efficiency

Very often you would not care about the actual value of these fixed effects. All you want is to remove the variance caused by these fixed effects from the model and reduce type 2 error (rejecting a significant relationship).

If that is all you care about, then you can simply remove these intercepts by "demeaning". Substracting the neighborhood average from each observation would essentially remove the effect of neighborhood.

Luckily you won't need to do this by hand. The excellent plm package handles fixed effects models nicely. Good thing, that you DO KNOW how to install and activate packages.

```{r}
rent2_plmFE_0 <- plm(rent ~ beds + baths + sqft, data = rent2,
                     # model="within" for fixed effects
                     # index = 'hood' for clustering variable
                     model = "within", index = "hood")
summary(rent2_plmFE_0)
```

If you compare the coefficients, you will see that the demeaned model is identical to dummy variables model.

There are many other ways of handling the violation of "Independence of Observations" assumption, basically anything more would turn into a full course rather than a learning activity. If you are interested, look into random effects, mixed effects, and Generalized Method of Moments models.

# Solutions to Exercises

1 - Calculate the intercept for neighborhoods A and C.

Intercept for Neighborhood A would be $\beta_0 =$ `r unname(round(rent2_lm_0$coefficients[1], digits=2))`.

The intercept for neighborhood C will then be $\beta_0 + \alpha_C =$ `r unname(round(rent2_lm_0$coefficients[1]+rent2_lm_0$coefficients[6], digits=2))`.